### 分布式

#### 分布式

##### Q1. 什么是分布式系统的副本一致性？有哪些？
分布式系统通过副本控制协议，使得从系统外部读取系统内部各个副本的数据在一定的约束条件下相同，称之为副本一致性(consistency)。副本一致性是针对分布式系统而言的，不是针对某一个副本而言。

1. 强一致性(strong consistency)：任何时刻任何用户或节点都可以读到最近一次成功更新的副本数据。强一致性是程度最高的一致性要求，也是实践中最难以实现的一致性。
2. 单调一致性(monotonic consistency)：任何时刻，任何用户一旦读到某个数据在某次更新后的值，这个用户不会再读到比这个值更旧的值。单调一致性是弱于强一致性却非常实用的一种一致性级别。因为通常来说，用户只关心从己方视角观察到的一致性，而不会关注其他用户的一致性情况。
3. 会话一致性(session consistency)：任何用户在某一次会话内一旦读到某个数据在某次更新后的值，这个用户在这次会话过程中不会再读到比这个值更旧的值。会话一致性通过引入会话的概念，在单调一致性的基础上进一步放松约束，会话一致性只保证单个用户单次会话内数据的单调修改，对于不同用户间的一致性和同一用户不同会话间的一致性没有保障。实践中有许多机制正好对应会话的概念，例如php 中的session 概念。
4. 最终一致性(eventual consistency)：最终一致性要求一旦更新成功，各个副本上的数据最终将达 到完全一致的状态，但达到完全一致状态所需要的时间不能保障。对于最终一致性系统而言，一个用户只要始终读取某一个副本的数据，则可以实现类似单调一致性的效果，但一旦用户更换读取的副本，则无法保障任何一致性
5. 弱一致性(week consistency)：一旦某个更新成功，用户无法在一个确定时间内读到这次更新的值，且即使在某个副本上读到了新的值，也不能保证在其他副本上可以读到新的值。弱一致性系统一般很难在实际中使用，使用弱一致性系统需要应用方做更多的工作从而使得系统可用。

##### Q2. 在分布式系统中有哪些常见的一致性算法？
1. 分布式算法 - 一致性Hash算法
   - 一致性Hash算法是个经典算法，Hash环的引入是为解决单调性(Monotonicity)的问题；虚拟节点的引入是为了解决平衡性(Balance)问题
2. 分布式算法 - Paxos算法
   - Paxos算法是Lamport宗师提出的一种基于消息传递的分布式一致性算法，使其获得2013年图灵奖。自Paxos问世以来就持续垄断了分布式一致性算法，Paxos这个名词几乎等同于分布式一致性, 很多分布式一致性算法都由Paxos演变而来
3. 分布式算法 - Raft算法
   - Paxos是出了名的难懂，而Raft正是为了探索一种更易于理解的一致性算法而产生的。它的首要设计目的就是易于理解，所以在选主的冲突处理等方式上它都选择了非常简单明了的解决方案
4. 分布式算法 - ZAB算法
   - ZAB 协议全称：Zookeeper Atomic Broadcast（Zookeeper 原子广播协议）, 它应该是所有一致性协议中生产环境中应用最多的了。为什么呢？因为他是为 Zookeeper 设计的分布式一致性协议！

##### Q3. 谈谈你对一致性hash算法的理解？
判定哈希算法好坏的四个定义:
1. 平衡性(Balance): 平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件。
2. 单调性(Monotonicity): 单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲加入到系统中。哈希的结果应能够保证原有已分配的内容可以被映射到原有的或者新的缓冲中去，而不会被映射到旧的缓冲集合中的其他缓冲区。
3. 分散性(Spread): 在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。 
4. 负载(Load): 负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同 的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。

#### 全局性唯一ID
##### Q1. 全局唯一ID有哪些实现方案？
常见的分布式ID生成方式，大致分类的话可以分为两类：
1. 一种是类DB型的，根据设置不同起始值和步长来实现趋势递增，需要考虑服务的容错性和可用性;
2. 另一种是类snowflake型，这种就是将64位划分为不同的段，每段代表不同的涵义，基本就是时间戳、机器ID和序列数。这种方案就是需要考虑时钟回拨的问题以及做一些 buffer的缓冲设计提高性能。

##### Q2. 数据库方式实现方案？有什么缺陷？
MySQL为例:
1. 我们将分布式系统中数据库的同一个业务表的自增ID设计成不一样的起始值，然后设置固定的步长，步长的值即为分库的数量或分表的数量。
2. 以MySQL举例，利用给字段设置auto_increment_increment和auto_increment_offset来保证ID自增。
   - auto_increment_offset：表示自增长字段从那个数开始，他的取值范围是1 .. 65535。
   - auto_increment_increment：表示自增长字段每次递增的量，其默认值是1，取值范围是1 .. 65535。
3. 缺点也很明显，首先它强依赖DB，当DB异常时整个系统不可用。
4. 虽然配置主从复制可以尽可能的增加可用性，但是数据一致性在特殊情况下难以保证。
5. 主从切换时的不一致可能会导致重复发号。还有就是ID发号性能瓶颈限制在单台MySQL的读写性能。

redis实现:
1. Redis实现分布式唯一ID主要是通过提供像 INCR 和 INCRBY 这样的自增原子命令，由于Redis自身的单线程的特点所以能保证生成的 ID 肯定是唯一有序的。
2. 但是单机存在性能瓶颈，无法满足高并发的业务需求，所以可以采用集群的方式来实现。集群的方式又会涉及到和数据库集群同样的问题，所以也需要设置分段和步长来实现。
3. 为了避免长期自增后数字过大可以通过与当前时间戳组合起来使用，另外为了保证并发和业务多线程的问题可以采用 Redis + Lua的方式进行编码，保证安全。
4. Redis 实现分布式全局唯一ID，它的性能比较高，生成的数据是有序的，对排序业务有利，但是同样它依赖于redis，需要系统引进redis组件，增加了系统的配置复杂性。
5. 当然现在Redis的使用性很普遍，所以如果其他业务已经引进了Redis集群，则可以资源利用考虑使用Redis来实现。

##### Q3. 雪花算法如何实现的？
Snowflake，雪花算法是由Twitter开源的分布式ID生成算法，以划分命名空间的方式将 64-bit位分割成多个部分，每个部分代表不同的含义。而 Java中64bit的整数是Long类型，所以在 Java 中 SnowFlake 算法生成的 ID 就是 long 来存储的。

1. 第1位占用1bit，其值始终是0，可看做是符号位不使用。
2. 第2位开始的41位是时间戳，41-bit位可表示2^41个数，每个数代表毫秒，那么雪花算法可用的时间年限是(1L<<41)/(1000L360024*365)=69 年的时间。
3. 中间的10-bit位可表示机器数，即2^10 = 1024台机器，但是一般情况下我们不会部署这么台机器。如果我们对IDC（互联网数据中心）有需求，还可以将 10-bit 分 5-bit 给 IDC，分5-bit给工作机器。这样就可以表示32个IDC，每个IDC下可以有32台机器，具体的划分可以根据自身需求定义。
4. 最后12-bit位是自增序列，可表示2^12 = 4096个数。

这样的划分之后相当于在一毫秒一个数据中心的一台机器上可产生4096个有序的不重复的ID。但是我们 IDC 和机器数肯定不止一个，所以毫秒内能生成的有序ID数是翻倍的。

![雪花算法示意图](../../../picture/distributed/arch-z-id-3.png)

##### Q4. 雪花算法有什么问题？有哪些解决思路？
1. 有哪些问题？
   - 时钟回拨问题；
   - 趋势递增，而不是绝对递增；
   - 不能在一台服务器上部署多个分布式ID服务；
2. 如何解决时钟回拨？以百度的UidGenerator为例，CachedUidGenerator方式主要通过采取如下一些措施和方案规避了时钟回拨问题和增强唯一性：
   - 自增列：UidGenerator的workerId在实例每次重启时初始化，且就是数据库的自增ID，从而完美的实现每个实例获取到的workerId不会有任何冲突。
   - RingBuffer：UidGenerator不再在每次取ID时都实时计算分布式ID，而是利用RingBuffer数据结构预先生成若干个分布式ID并保存。
   - 时间递增：传统的雪花算法实现都是通过System.currentTimeMillis()来获取时间并与上一次时间进行比较，这样的实现严重依赖服务器的时间。而UidGenerator的时间类型是AtomicLong，且通过incrementAndGet()方法获取下一次的时间，从而脱离了对服务器时间的依赖，也就不会有时钟回拨的问题

#### 分布式锁
##### Q1. 有哪些方案实现分布式锁？
综合讲讲方案：
1. 使用场景
   - 需要保证一个方法在同一时间内只能被同一个线程执行
2. 实现方式:
   - 加锁和解锁
3. 方案,考虑因素(性能,稳定,实现难度,死锁)
   - 基于数据库做分布式锁--乐观锁(基于版本号)和悲观锁(基于排它锁)
   - 基于 redis 做分布式锁:setnx(key,当前时间+过期时间)和Redlock机制
   - 基于 zookeeper 做分布式锁:临时有序节点来实现的分布式锁,Curator
   - 基于 Consul 做分布式锁

##### Q2. 基于数据库如何实现分布式锁？有什么缺陷？
1. 基于数据库表（锁表，很少使用）
   - 最简单的方式可能就是直接创建一张锁表，然后通过操作该表中的数据来实现了。当我们想要获得锁的时候，就可以在该表中增加一条记录，想要释放锁的时候就删除这条记录。
2. 基于悲观锁: 悲观锁实现思路？
   - 在对任意记录进行修改前，先尝试为该记录加上排他锁（exclusive locking）。
   - 如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常。 具体响应方式由开发者根据实际需要决定。
   - 如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了。
   - 其间如果有其他对该记录做修改或加排他锁的操作，都会等待我们解锁或直接抛出异常。
3. 以MySQL InnoDB中使用悲观锁为例？
   - 要使用悲观锁，我们必须关闭mysql数据库的自动提交属性，因为MySQL默认使用autocommit模式，也就是说，当你执行一个更新操作后，MySQL会立刻将结果进行提交。set autocommit=0;
4. 基于乐观锁
   - 乐观并发控制（又名“乐观锁”，Optimistic Concurrency Control，缩写“OCC”）是一种并发控制的方法。
   - 它假设多用户并发的事务在处理时不会彼此互相影响，各事务能够在不产生锁的情况下处理各自影响的那部分数据
   - 在提交数据更新之前，每个事务会先检查在该事务读取数据后，有没有其他事务又修改了该数据
   - 如果其他事务有更新的话，正在提交的事务会进行回滚。
5. 以使用版本号实现乐观锁为例？
   - 使用版本号时，可以在数据初始化时指定一个版本号，每次对数据的更新操作都对版本号执行+1操作。并判断当前版本号是不是该数据的最新的版本号。
   - 需要注意的是，乐观锁机制往往基于系统中数据存储逻辑，因此也具备一定的局限性。
   - 由于乐观锁机制是在我们的系统中实现的，对于来自外部系统的用户数据更新操作不受我们系统的控制，因此可能会造成脏数据被更新到数据库中
   - 在系统设计阶段，我们应该充分考虑到这些情况，并进行相应的调整（如将乐观锁策略在数据库存储过程中实现，对外只开放基于此存储过程的数据更新途径，而不是将数据库表直接对外公开）。
6. 缺陷
   - 对数据库依赖，开销问题，行锁变表锁问题，无法解决数据库单点和可重入的问题。

##### Q3. 基于redis如何实现分布式锁？有什么缺陷？
1. 最基本的Jedis方案；加锁： set NX PX + 重试 + 重试间隔
   - 向Redis发起如下命令: SET productId:lock 0xx9p03001 NX PX 30000 
   - 其中，"productId"由自己定义，可以是与本次业务有关的id，
   - "0xx9p03001"是一串随机值，必须保证全局唯一(原因在后文中会提到)
   - “NX"指的是当且仅当key(也就是案例中的"productId:lock”)在Redis中不存在时，返回执行成功，否则执行失败
   - "PX 30000"指的是在30秒后，key将被自动删除。执行命令后返回成功，表明服务成功的获得了锁
2. 解锁： 采用lua脚本： 在删除key之前，一定要判断服务A持有的value与Redis内存储的value是否一致。如果贸然使用服务A持有的key来删除锁，则会误将服务B的锁释放掉。
3. 基于RedLock实现分布式锁: 假设有两个服务A、B都希望获得锁，有一个包含了5个redis master的Redis Cluster，执行过程大致如下:
   - 客户端获取当前时间戳，单位: 毫秒
   - 服务A轮寻每个master节点，尝试创建锁。(这里锁的过期时间比较短，一般就几十毫秒) RedLock算法会尝试在大多数节点上分别创建锁，假如节点总数为n，那么大多数节点指的是n/2+1。
   - 客户端计算成功建立完锁的时间，如果建锁时间小于超时时间，就可以判定锁创建成功。如果锁创建失败，则依次(遍历master节点)删除锁。
   - 只要有其它服务创建过分布式锁，那么当前服务就必须轮寻尝试获取锁。
4. 基于Redisson实现分布式锁？
   - 过程？
     - 线程去获取锁，获取成功: 执行lua脚本，保存数据到redis数据库。
     - 线程去获取锁，获取失败: 订阅了解锁消息，然后再尝试获取锁，获取成功后，执行lua脚本，保存数据到redis数据库。
   - 互斥？
     - 如果这个时候客户端B来尝试加锁，执行了同样的一段lua脚本。
     - 第一个if判断会执行“exists myLock”，发现myLock这个锁key已经存在
     - 接着第二个if判断，判断myLock锁key的hash数据结构中，是否包含客户端B的ID，但明显没有，那么客户端B会获取到pttl myLock返回的一个数字，代表myLock这个锁key的剩余生存时间
     - 此时客户端B会进入一个while循环，不听的尝试加锁。
   - watch dog自动延时机制？
     - 客户端A加锁的锁key默认生存时间只有30秒，如果超过了30秒，客户端A还想一直持有这把锁，怎么办？
     - 其实只要客户端A一旦加锁成功，就会启动一个watch dog看门狗，它是一个后台线程，会每隔10秒检查一下，如果客户端A还持有锁key，那么就会不断的延长锁key的生存时间。
   - 可重入？
     - 每次lock会调用incerby，每次unlock会减一。
5. 方案比较
   - 借助Redis实现分布式锁时，有一个共同的缺陷: 当获取锁被决绝后，需要不断的循环，重新发送获取锁(创建key)的请求，直到请求成功。这就造成空转，浪费宝贵的CPU资源。
   - RedLock算法本身有争议，并不能保证健壮性。
   - Redisson实现分布式锁时，除了将key新增到某个指定的master节点外，还需要由master自动异步的将key和value等数据同步至绑定的slave节点上。
   - 那么问题来了，如果master没来得及同步数据，突然发生宕机，那么通过故障转移和主备切换，slave节点被迅速升级为master节点，新的客户端加锁成功，旧的客户端的watch dog发现key存在，误以为旧客户端仍然持有这把锁，这就导致同时存在多个客户端持有同名锁的问题了。

#### 分布式事务

##### Q1. 什么是ACID？
一个事务有四个基本特性，也就是我们常说的（ACID）：
1. Atomicity（原子性）：事务是一个不可分割的整体，事务内所有操作要么全做成功，要么全失败。
2. Consistency（一致性）：事务执行前后，数据从一个状态到另一个状态必须是一致的（A向B转账，不能出现A扣了钱，B却没收到）。
3. Isolation（隔离性）： 多个并发事务之间相互隔离，不能互相干扰。
4. Durability（持久性）：事务完成后，对数据库的更改是永久保存的，不能回滚。

#### 分布式缓存

##### Q1. 分布式系统中常用的缓存方案有哪些？
1. 客户端缓存：页面和浏览器缓存，APP缓存，H5缓存，localStorage和sessionStorage
2. CDN缓存：
   - 内存存储：数据的缓存
   - 内容分发：负载均衡
3. nginx缓存：本地缓存，外部缓存
4. 数据库缓存：持久层缓存（mybatis，hibernate多级缓存），Mysql查询缓存
5. 操作系统缓存：Page Cache，Buffer Cache

##### Q2. 分布式系统缓存的更新模式？
1. Cache Aside模式
   - 读取失效：cache数据没有命中，查询DB，成功后把数据写入缓存
   - 读取命中：读取cache数据
   - 更新：把数据更新到DB，失效缓存
2. Read/Write Through模式
   - 缓存代理了DB读取、写入的逻辑，可以把缓存看成唯一的存储。
3. Write Back模式
   - 这种模式下所有的操作都走缓存，缓存里的数据再通过异步的方式同步到数据库里面。所以系统的写性能能够大大提升了。

##### Q3. 分布式系统缓存淘汰策略
缓存淘汰，又称为缓存逐出(cache replacement algorithms或者cache replacement policies)，是指在存储空间不足的情况下，缓存系统主动释放一些缓存对象获取更多的存储空间。一般LRU用的比较多，可以重点了解一下。

1. FIFO 先进先出（First In First Out）是一种简单的淘汰策略，缓存对象以队列的形式存在，如果空间不足，就释放队列头部的（先缓存）对象。一般用链表实现。
2. LRU 最近最久未使用（Least Recently Used），这种策略是根据访问的时间先后来进行淘汰的，如果空间不足，会释放最久没有访问的对象（上次访问时间最早的对象）。比较常见的是通过优先队列来实现。
3. LFU 最近最少使用（Least Frequently Used），这种策略根据最近访问的频率来进行淘汰，如果空间不足，会释放最近访问频率最低的对象。这个算法也是用优先队列实现的比较常见。
